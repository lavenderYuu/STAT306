---
title: "WeightChange"
output: html_document
date: "2024-11-24"
---

```{r setup, include=FALSE}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(leaps)
library(dplyr)
library(ggplot2)
library(car)
library(reshape2)
```

## Load Data

```{r}
weightData <- read.table("weight_change_dataset.csv", header=TRUE, sep=",")
head(weightData)
```

## Data Cleaning

```{r}

#we treate sleep quality and stress level treat as continuous variables since the levels have a meaningful progression or rank (e.g., increasing physical effort or impact on health).

cleanWeight <- weightData %>%
  mutate(
    Sleep.Quality = case_when(
      Sleep.Quality == "Excellent" ~ 4,
      Sleep.Quality == "Good" ~ 3,
      Sleep.Quality == "Fair" ~ 2,
      Sleep.Quality == "Poor" ~ 1
    )
  )


colnames(cleanWeight) <- c(
  "ParticipantID", "Age", "Gender", 
  "CurrentWeightLbs", "BMRCalories", "DailyCaloriesConsumed", 
  "DailyCaloricSurplusDeficit", "WeightChangeLbs", "DurationWeeks", 
  "PhysicalActivityLevel", "SleepQuality", "StressLevel", 
  "FinalWeightLbs"
)

# Convert categorical variables into factors
cleanWeight <- cleanWeight %>%
  mutate(
    Gender = factor(Gender, levels = c("M", "F"))
  )

#make ratio by weight and duration
#by gradescope suggestion 3

cleanWeight <- cleanWeight %>%
  mutate(weightChangePerWeek = WeightChangeLbs / DurationWeeks) %>%
  select(-ParticipantID, -WeightChangeLbs, -DurationWeeks)

head(cleanWeight)
```

## Check multicollinearity

```{r}
numeric_data <- cleanWeight %>%
  select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Melt the correlation matrix for visualization
cor_melt <- melt(cor_matrix)

# Create the heatmap
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap with Values", x = "", y = "")
```

```{r}
cleanWeight <- cleanWeight %>%
  select(-CurrentWeightLbs, -FinalWeightLbs) 

#QUESTION: should I exclude Final weight?
```

```{r}

numeric_data <- cleanWeight %>%
  select(where(is.numeric))
# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Melt the correlation matrix for visualization
cor_melt <- melt(cor_matrix)

# Create the heatmap
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap with Values", x = "", y = "")
```

```{r}
vif_model <- lm(weightChangePerWeek ~ ., data = cleanWeight)

# Calculate VIF
vif_values <- vif(vif_model)

# Print VIF values
print(vif_values)
```

```{r}
#Incredible High VIF mulicolinearly so delete one vairable
cleanWeight <- cleanWeight %>%
  select(-DailyCaloricSurplusDeficit)


vif_model <- lm(weightChangePerWeek ~ ., data = cleanWeight)

# Calculate VIF
vif_values <- vif(vif_model)

# Print VIF values
print(vif_values)
```

-   **VIF \< 5**: Low multicollinearity.

-   **VIF 5â€“10**: Moderate multicollinearity (consider investigating further).

-   **VIF \> 10**: High multicollinearity (may require remedial measures, such as removing variables or PCA).

# Start Fiting

## Simple linear model method Added based on lab 8

This is one way ignored the quadratic, cubic, interaction...

```{r}

s = regsubsets(weightChangePerWeek~., data=cleanWeight, method="exhaustive")
ss <- summary(s)
ss
```

```{r}
num_predictors <- rownames(ss$which)


plot(num_predictors, ss$rsq, type = "b", pch = 19,
     main = "R^2 vs Number of Predictors",
     xlab = "Number of Predictors", ylab = "R^2")

# Plot Adjusted R^2
plot(num_predictors, ss$adjr2, type = "b", pch = 19,
     main = "Adjusted R^2 vs Number of Predictors",
     xlab = "Number of Predictors", ylab = "Adjusted R^2")


num_predictors <- 1:length(ss$cp) 
cp_values <- ss$cp

# Plot Cp 
plot(num_predictors, cp_values, type = "b", pch = 19,
     main = "Mallows' Cp vs Number of Predictors",
     xlab = "Number of Predictors", ylab = "Cp")

abline(a = 1, b = 1, col = "royalblue", lty = 2, lwd = 2)

text(num_predictors, cp_values, labels = round(cp_values, 2), pos = 3, cex = 0.8, col = "black")

```
We look at the scatterplots to see if there might be an interaction between the variables.

```{r}
ggplot(cleanWeight, aes(x = SleepQuality, y = weightChangePerWeek, color = as.factor(PhysicalActivityLevel))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "PhysicalActivityLevel and SleepQuality",
       x = "Sleep Quality",
       y = "Weight Change Per Week")
```
```{r}
ggplot(cleanWeight, aes(x = StressLevel, y = weightChangePerWeek, color = as.factor(PhysicalActivityLevel))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "PhysicalActivityLevel and SleepQuality",
       x = "Stress Level",
       y = "Weight Change Per Week")
```
The slope of the fitted line does not change significantly, and we can test the significance of the interaction term to determine whether to include interaction.

```{r}
regLab <- lm(weightChangePerWeek ~ PhysicalActivityLevel * StressLevel + SleepQuality ,  cleanWeight)
summary(regLab)
```
```{r}
regLab <- lm(weightChangePerWeek ~ PhysicalActivityLevel * SleepQuality +  StressLevel,  cleanWeight)
summary(regLab)
```
All of the interaction terms are not significant, so we don't include the interaction.

```{r}
regLab <- lm(weightChangePerWeek ~ PhysicalActivityLevel + SleepQuality +  StressLevel,  cleanWeight)
summary(regLab)
```
The model shows that the variable Physical Activity Level is statistically insignificant, and we look at the various in weight Change PerWeek for Physical Activity Level via boxplot.

```{r}
ggplot(cleanWeight, aes(x = as.factor(PhysicalActivityLevel), y = weightChangePerWeek, fill = as.factor(PhysicalActivityLevel))) +
  geom_boxplot() +
  labs(title = "Boxplot of Weight Change Per Week by Physical Activity Level",
       x = "Physical Activity Level",
       y = "Weight Change Per Week") +
  scale_fill_brewer(palette = "Blues") +
  theme_minimal()
```
With boxplot we can see that the median difference between levels is not large, which may be the reason why Physical Activity Level is not statistically significant.

```{r}
ggplot(regLab, aes(x = regLab$fitted.values, y = regLab$residuals)) +
  geom_point() +
  labs(title = "Residuals vs. Fitted value",
       x = "Fitted Value", y = "Residuals") +
  theme_minimal()

```
```{r}
regLab_log <- lm(log(weightChangePerWeek + 3) ~ PhysicalActivityLevel + SleepQuality +  StressLevel,  cleanWeight)
summary(regLab_log)
```
```{r}
ggplot(regLab_log, aes(x = regLab_log$fitted.values, y = regLab_log$residuals)) +
  geom_point() +
  labs(title = "Residuals vs. Fitted value",
       x = "Fitted Value", y = "Residuals") +
  theme_minimal()
```
```{r}
ggplot(data = cleanWeight, aes(x = SleepQuality, y = weightChangePerWeek)) +
  geom_point() +
  labs(title = "Linear vs Quadratic Fit", x = "Sleep Quality", y = "weight Change Per Week")

ggplot(data = cleanWeight, aes(x = StressLevel, y = weightChangePerWeek)) +
  geom_point() +
  labs(title = "Linear vs Quadratic Fit", x = "Stress Level", y = "weight Change Per Week")
```
The scatterplot shows that Sleep Quality, Stress Level and weight Change Per Week do not have an obvious quadratic shape, so we still choose the linear model.

```{r}
n <- nrow(cleanWeight)
squared_errors <- numeric(n)
for (i in 1:n) {
  train_data <- cleanWeight[-i, ]
  test_data <- cleanWeight[i, , drop = FALSE]
  
  predicted <- predict(regLab, newdata = test_data)
  
  squared_errors[i] <- (test_data$weightChangePerWeek - predicted)^2
}

loocv_mse <- mean(squared_errors)
cat("LOOCV Mean Squared Error:", loocv_mse, "\n")
```

# Not So sure, start now

## Start fitting, and eliminate variables

How should I deal with interaction, cubic things

### Figure out interaction based on plot and test

This is second way, test-based, eliminate according to p-value

```{r}
predictors <- c("DailyCaloriesConsumed", "BMRCalories","PhysicalActivityLevel", "Age","SleepQuality", "StressLevel")

# Plot each predictor
for (var in predictors) {
  print(
    ggplot(cleanWeight, aes_string(x = var, y = "weightChangePerWeek", color = "Gender")) +
      geom_point(alpha = 0.6) +
      labs(title = paste("Interaction:", var, "vs Weight Change by Gender"),
           x = var, y = "Weight Change per Week") +
      theme_minimal()
  )
}
#It seems BMRCalories will have different trend relative to gender
```

```{r}
regWithInteraction <- lm(weightChangePerWeek ~ Age + Gender * BMRCalories + DailyCaloriesConsumed + PhysicalActivityLevel + StressLevel + SleepQuality,  cleanWeight)
regWithoutInteraction <- lm(weightChangePerWeek ~ . , cleanWeight)

```

```{r}
summary(regWithoutInteraction)
#according to the p value the interaction is useless
```


```{r}
summary(regWithInteraction)
```


## Finish for interaction, start dealing which variable should left

based on p-value

```{r}
reg1 <- lm(weightChangePerWeek ~ Age + Gender + BMRCalories + DailyCaloriesConsumed + StressLevel + SleepQuality, cleanWeight)
summary(reg1)
```

```{r}
reg2 <- lm(weightChangePerWeek ~ Gender + BMRCalories + DailyCaloriesConsumed + StressLevel + SleepQuality, cleanWeight)
summary(reg2)
```

```{r}

reg3 <- lm(weightChangePerWeek ~  Gender + DailyCaloriesConsumed + StressLevel + SleepQuality , cleanWeight)
summary(reg3)
```

```{r}
RSS_model <- sum(resid(reg2)^2)         
sigma2 <- summary(reg2)$sigma^2      
p <- length(coef(reg2))                
n <- nrow(cleanWeight)        

Cp <- (RSS_model / sigma2) + 2 * p - n
#valid CP 5+1

par(mfrow = c(2, 2)) 
plot(reg2)
```

```{r}

RSS_model <- sum(resid(reg3)^2)         
sigma2 <- summary(reg3)$sigma^2      
p <- length(coef(reg3))                
n <- nrow(cleanWeight)        

Cp <- (RSS_model / sigma2) + 2 * p - n
#Valid CP 4+1

par(mfrow = c(2, 2)) 
plot(reg3)
```

### Previous residual plot seems shows some pattern, Start thinking about other model fit, such cubic, quadratic

```{r}

meanDCC = mean(cleanWeight$DailyCaloriesConsumed)
reg_cubic <- lm(weightChangePerWeek ~ Gender + 
                 I((meanDCC-DailyCaloriesConsumed)^3) + SleepQuality + StressLevel,  data = cleanWeight)

summary(reg_cubic)
```

```{r}

par(mfrow = c(2, 2)) 
plot(reg_cubic)
```

## Validation

```{r}
squared_errors <- numeric(n)

# Perform LOOCV
for (i in 1:n) {
  train_data <- cleanWeight[-i, ]
  test_data <- cleanWeight[i, , drop = FALSE]
  loocv_model <- lm(weightChangePerWeek ~ Gender + I((meanDCC - DailyCaloriesConsumed)^3) + SleepQuality + StressLevel, data = train_data)
  predicted <- predict(loocv_model, newdata = test_data)
  squared_errors[i] <- (test_data$weightChangePerWeek - predicted)^2
}

loocv_mse <- mean(squared_errors)
cat("LOOCV Mean Squared Error:", loocv_mse, "\n")
```

LOOCV is a test that treat 1 data as the test set and 99 as the training set,

since we have 100 data, so each data has a chance to become the "test set"
